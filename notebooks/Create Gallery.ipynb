{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import os\n",
      "import cv2\n",
      "import startup\n",
      "import config\n",
      "from dataset.dataset import DataSet\n",
      "from algorithms.analysisframework       import AnalysisFramework\n",
      "from algorithms.distance import *\n",
      "from utils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n",
        "/media/blcv/drive_2TB/CODE/FV-Benchmark/lib"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "IDEA: Get best image per class. The score for each image is measured by: sum of similarity to other images. \n",
      "Flow:\n",
      "1. Get labels file and divide it to classes\n",
      "2. Create Verification task, each instance of class in compared to other instaces in same classes\n",
      "3. Sum the similarity score for each instance, sort by score\n",
      "4. Get best image per class\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "'\\nIDEA: Get best image per class. The score for each image is measured by: sum of similarity to other images. \\nFlow:\\n1. Get labels file and divide it to classes\\n2. Create Verification task, each instance of class in compared to other instaces in same classes\\n3. Sum the similarity score for each instance, sort by score\\n4. Get best image per class\\n'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read data associated with Net\n",
      "label_file = \"/media/blcv/drive_2TB/CODE/FV-Benchmark/DataSets/Felix/labels.txt\"\n",
      "main_folder = \"/media/blcv/drive_2TB/CODE/FV-Benchmark/\"\n",
      "config_file = main_folder + \"Nets/configs_net/casia_ver_conv52_correct_felix.json\"\n",
      "config_file = parse_deep_config(config_file)\n",
      "data = DataSet(\"deep\",config_file)\n",
      "labels  = data.labels\n",
      "labels_set = set(labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#divide image per class\n",
      "per_class = dict()\n",
      "for elem in labels_set:\n",
      "    per_class[str(elem)] = list()\n",
      "#collect indexes of instances of classes    \n",
      "for idx,elem in enumerate(labels):\n",
      "    per_class[str(elem)].append(idx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create verification task:\n",
      "verification_task = list()\n",
      "for label_idx in labels_set:\n",
      "    class_collection  = per_class[str(label_idx)]\n",
      "    for elem_main in class_collection:\n",
      "        for elem in class_collection:\n",
      "            verification_task.append((elem_main, elem, 1 ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(verification_task)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "140822757"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.save(\"verification_felix.npy\", verification_task)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#After calulcating matches outside iPython (using parallel etc.), load score and extract best images\n",
      "score_file = \"/media/blcv/drive_2TB/CODE/FV-Benchmark/FELIX_data/calculate_matching/divide_pairs/scores_all.txt\"\n",
      "with open(score_file, 'r') as f:\n",
      "    lines_score = [line.strip() for line in f]\n",
      "lines_score = sorted(lines_score,key=lambda x: int(x.split('/')[-1].split('.')[0].split(\"_\")[1]),  reverse=False)\n",
      "\n",
      "# scores = np.load(\"scores_verification_felix.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verification_task = np.load(\"verification_felix.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores_matrix = np.zeros((verification_task.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_elem = 0\n",
      "for idx, elem in enumerate(lines_score):\n",
      "    data = np.load(elem)\n",
      "    scores_matrix[start_elem: start_elem + data.shape[0]] = data\n",
      "    start_elem += data.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#acculumate scores per class per instance\n",
      "accumulate_score = dict()\n",
      "for elem in labels_set:\n",
      "    accumulate_score[str(elem)] = dict()\n",
      "    \n",
      "num = 0    \n",
      "for elem, score in zip(verification_task, scores_matrix):\n",
      "    print num\n",
      "    num += 1\n",
      "    idx = elem[0]\n",
      "    label_idx = labels[idx]\n",
      "    if str(idx) not in  accumulate_score[str(label_idx)].keys():\n",
      "      accumulate_score[str(label_idx)][str(idx)] = 0\n",
      "    accumulate_score[str(label_idx)][str(idx)] += score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-30a49e2227f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabel_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccumulate_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maccumulate_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36marray_str\u001b[0;34m(a, max_line_width, precision, suppress_small)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \"\"\"\n\u001b[0;32m-> 1715\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_string_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/arrayprint.pyc\u001b[0m in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The `_format` attribute is deprecated in Numpy \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                   \u001b[0;34m\"2.0 and will be removed in 2.1. Use the \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load accumulated result\n",
      "accumulate_score = load_cPickle(\"/media/blcv/drive_2TB/CODE/FV-Benchmark/FELIX_data/create_galery/acc_score.dict\")\n",
      "label_file = \"/media/blcv/drive_2TB/CODE/FV-Benchmark/DataSets/Felix/labels.txt\"\n",
      "with open(label_file, 'r') as f:\n",
      "    label_file_raw = [line.strip() for line in f]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get best image per class\n",
      "best_image_per_clas = dict()\n",
      "best_image = list()\n",
      "for elem in labels_set:  \n",
      "    images_classes = accumulate_score[str(elem)].items()\n",
      "    images_classes = sorted(images_classes,key=lambda x: x[1],  reverse=True)\n",
      "    best_image_per_clas[str(elem)] = label_file_raw[int(images_classes[0][0])]\n",
      "    best_image.append(label_file_raw[int(images_classes[0][0])])\n",
      "    \n",
      "with open(\"gallery_felix.txt\", 'w') as f:\n",
      "    f.writelines(\"%s\\n\" % line for line in best_image)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}